---
title: "Efficient Training of Large Language Models: A Survey"
publication_date: 2023-12-10
authors:
  - title: Zhangyin Feng
    organization: peking-university/_index
  - title: Yanghua Jin
    organization: microsoft-research-asia/_index
  - title: Xiaojun Wan
    organization: peking-university/_index
categories:
  - sustainable/_index
  - ai/_index
tags:
  - Energy efficiency
  - Language models
  - Model training
  - Green AI
  - Resource optimization
resource_type: research
summary: |
  This comprehensive survey examines various approaches to make the training of large language models more efficient and environmentally sustainable.

  The research analyzes different techniques including model compression, efficient attention mechanisms, and hardware-aware training strategies that can significantly reduce the computational and energy costs.

  The authors provide a systematic comparison of different efficiency methods and their impact on model performance, training time, and energy consumption.
source_url: https://arxiv.org/abs/2312.03863
source_document: https://arxiv.org/pdf/2312.03863.pdf
source_organizations:
  - peking-university/_index
  - microsoft-research-asia/_index
language: en
--- 