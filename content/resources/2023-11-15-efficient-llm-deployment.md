---
title: "Efficient Large Language Model Deployment: A Survey and Empirical Study"
publication_date: 2023-11-15
authors:
  - title: Xiaonan Nie
    organization: peking-university/_index
  - title: Xixuan Zhang
    organization: microsoft-research-asia/_index
  - title: Shuo Wang
    organization: microsoft-research-asia/_index
  - title: Xuanyu Zhu
    organization: peking-university/_index
categories:
  - sustainable/_index
  - ai/_index
tags:
  - LLM deployment
  - Energy efficiency
  - Model optimization
  - Green AI
  - Resource utilization
resource_type: research
summary: |
  This comprehensive survey investigates various approaches for deploying large language models efficiently, focusing on reducing computational resources and energy consumption.

  The research evaluates different deployment strategies including model compression, quantization, and hardware acceleration techniques, providing empirical evidence of their effectiveness.

  The authors present a systematic comparison of deployment methods and their impact on model performance, latency, and energy usage.
source_url: https://arxiv.org/abs/2311.09541
source_document: https://arxiv.org/pdf/2311.09541.pdf
source_organizations:
  - peking-university/_index
  - microsoft-research-asia/_index
language: en
--- 