 ---
title: Energy and Policy Considerations for Deep Learning in NLP
publication_date: 2019-06-05
authors:
  - title: Emma Strubell
    organization: university-of-massachusetts-amherst/_index
  - title: Ananya Ganesh
    organization: university-of-massachusetts-amherst/_index
  - title: Andrew McCallum
    organization: university-of-massachusetts-amherst/_index
categories:
  - sustainable/_index
tags:
  - environmental impact
  - AI
  - NLP
  - Carbon footprint
resource_type: research
summary: |
  This pioneering study examines the carbon footprint of training natural language processing models. The authors quantify the financial and environmental costs of training various NLP models.

  The study reveals that training a single BERT model can emit as much CO2 as a trans-Atlantic flight, and that the computational costs of NLP models double every 3-4 months.

  The authors provide concrete recommendations to reduce environmental impact, particularly by prioritizing energy efficiency in model design and using renewable energy sources for training.
source_url: https://aclanthology.org/P19-1355/
source_document: https://arxiv.org/pdf/1906.02243.pdf
source_organizations:
  - university-of-massachusetts-amherst/_index
language: en
---